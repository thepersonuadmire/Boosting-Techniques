{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNJZs171rbEU/5QYgQnGk53",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thepersonuadmire/Boosting-Techniques/blob/main/Boosting_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Theoretical"
      ],
      "metadata": {
        "id": "VOdD8BkN1pJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Boosting in Machine Learning?\n"
      ],
      "metadata": {
        "id": "AeIAt84112O_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boosting is an ensemble learning technique that combines multiple weak learners (often decision trees) to create a strong learner. The models are trained sequentially, with each new model focusing on the errors made by the previous ones. The final prediction is made by aggregating the predictions of all models, typically through a weighted sum."
      ],
      "metadata": {
        "id": "tXNRUWiG7xZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. How does Boosting differ from Bagging?\n"
      ],
      "metadata": {
        "id": "dsulLf2Z2ig9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boosting: Models are trained sequentially, where each model attempts to correct the errors of its predecessor. It focuses on misclassified instances and adjusts their weights.\n",
        "\n",
        "Bagging: Models are trained independently and in parallel using different subsets of the training data. It aims to reduce variance by averaging the predictions of multiple models.\n"
      ],
      "metadata": {
        "id": "8fQah9ml70r0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the key idea behind AdaBoost?\n"
      ],
      "metadata": {
        "id": "I6eRyu7v2vvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaBoost (Adaptive Boosting) assigns weights to each instance in the training set. Initially, all instances have equal weights, but after each iteration, the weights of misclassified instances are increased, while those of correctly classified instances are decreased. This allows subsequent models to focus more on difficult cases."
      ],
      "metadata": {
        "id": "s67P__us77Ov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Explain the working of AdaBoost with an example.\n"
      ],
      "metadata": {
        "id": "Kul3n03A2yYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start with equal weights for all training instances.\n",
        "\n",
        "Train a weak classifier (e.g., a decision stump).\n",
        "\n",
        "Calculate the error rate and update the weights based on misclassifications.\n",
        "\n",
        "Train the next weak classifier on the updated weights.\n",
        "\n",
        "Repeat the process for a specified number of iterations or until no further improvement is observed.\n",
        "\n",
        "Combine the weak classifiers into a final strong classifier using a weighted majority vote.\n",
        "\n",
        "Example: Suppose we have a dataset with 5 instances. After the first classifier, 2 instances are misclassified. The weights of these misclassified instances are increased, and the weights of the correctly classified instances are decreased. The next classifier will focus more on the misclassified instances."
      ],
      "metadata": {
        "id": "ahhB-KUd8BTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Gradient Boosting, and how is it different from AdaBoost?\n"
      ],
      "metadata": {
        "id": "CBNk_CcA2_12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting builds models sequentially, where each new model is trained to predict the residuals (errors) of the combined ensemble of previous models. Unlike AdaBoost, which adjusts weights based on misclassifications, Gradient Boosting minimizes a loss function directly."
      ],
      "metadata": {
        "id": "lFQM8vGr8Hpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the loss function in Gradient Boosting?\n"
      ],
      "metadata": {
        "id": "VevckSKY3B1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss function in Gradient Boosting can vary depending on the task (e.g., mean squared error for regression, log loss for classification). The goal is to minimize this loss function by fitting new models to the residuals of the predictions."
      ],
      "metadata": {
        "id": "vgT0fskM8QrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How does XGBoost improve over traditional Gradient Boosting?\n"
      ],
      "metadata": {
        "id": "6-N3gQz53FQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost (Extreme Gradient Boosting) introduces several enhancements:\n",
        "\n",
        "Regularization to prevent overfitting.\n",
        "\n",
        "Parallel processing for faster computation.\n",
        "\n",
        "Tree pruning to optimize the structure of trees.\n",
        "\n",
        "Handling missing values natively."
      ],
      "metadata": {
        "id": "QfIH2Qco8Tbi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the difference between XGBoost and CatBoost?\n"
      ],
      "metadata": {
        "id": "MKbus04b3HGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost: Primarily designed for numerical features and requires preprocessing for categorical features.\n",
        "\n",
        "CatBoost: Specifically designed to handle categorical features without extensive preprocessing, using techniques like ordered boosting to reduce overfitting."
      ],
      "metadata": {
        "id": "rw3-BHK18YxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are some real-world applications of Boosting techniques?\n"
      ],
      "metadata": {
        "id": "bezCkM1A3Ixb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fraud detection in finance.\n",
        "\n",
        "Customer churn prediction.\n",
        "\n",
        "Image classification.\n",
        "\n",
        "Natural language processing tasks like sentiment analysis."
      ],
      "metadata": {
        "id": "T6Yq40CU8cYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How does regularization help in XGBoost?\n"
      ],
      "metadata": {
        "id": "P6N-Nhlb3K6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization helps to control the complexity of the model, preventing overfitting. XGBoost includes L1 (Lasso) and L2 (Ridge) regularization terms in its objective function, which penalize large coefficients."
      ],
      "metadata": {
        "id": "l98VkJMg8ijJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are some hyperparameters to tune in Gradient Boosting models?\n"
      ],
      "metadata": {
        "id": "vTr1zPxM3MjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning rate (shrinkage).\n",
        "\n",
        "Number of estimators (trees).\n",
        "\n",
        "Maximum depth of trees.\n",
        "\n",
        "Minimum samples split.\n",
        "\n",
        "Subsample ratio."
      ],
      "metadata": {
        "id": "EqPQ4nqx8mfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the concept of Feature Importance in Boosting?\n"
      ],
      "metadata": {
        "id": "YqvLjzHO3OVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature importance measures how much each feature contributes to the model's predictions. In boosting, it can be derived from the number of times a feature is used to split the data across all trees or the total reduction in loss attributed to that feature."
      ],
      "metadata": {
        "id": "70adKl4m8rO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Why is CatBoost efficient for categorical data?"
      ],
      "metadata": {
        "id": "cufu3PN43Qew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CatBoost uses a technique called \"ordered boosting\" and can handle categorical features directly without the need for one-hot encoding or label encoding, which reduces the risk of overfitting and improves performance."
      ],
      "metadata": {
        "id": "DsgnjY8Q8uU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "Ia9xC9SM3S1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Train an AdaBoost Classifier on a sample dataset and print model accuracy.\n"
      ],
      "metadata": {
        "id": "Nxb51I8j2gCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost Classifier\n",
        "ada_classifier = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50)\n",
        "ada_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = ada_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"AdaBoost Classifier Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "HX8YtkrM89hG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE).\n"
      ],
      "metadata": {
        "id": "mAgTcYTm3nri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Create a sample regression dataset\n",
        "X_reg, y_reg = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost Regressor\n",
        "ada_regressor = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=1), n_estimators=50)\n",
        "ada_regressor.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Predict and evaluate MAE\n",
        "y_reg_pred = ada_regressor.predict(X_test_reg)\n",
        "mae = mean_absolute_error(y_test_reg, y_reg_pred)\n",
        "print(f\"AdaBoost Regressor MAE: {mae:.2f}\")"
      ],
      "metadata": {
        "id": "NalHVQ_v9Au2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance.\n"
      ],
      "metadata": {
        "id": "rz-pIcuW3qIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer_data = load_breast_cancer()\n",
        "X_cancer, y_cancer = cancer_data.data, cancer_data.target\n",
        "\n",
        "# Train Gradient Boosting Classifier\n",
        "gb_classifier = GradientBoostingClassifier(n_estimators=100)\n",
        "gb_classifier.fit(X_cancer, y_cancer)\n",
        "\n",
        "# Print feature importance\n",
        "feature_importance = gb_classifier.feature_importances_\n",
        "print(\"Feature Importance:\", feature_importance)"
      ],
      "metadata": {
        "id": "kclx706U9Eb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Train a Gradient Boosting Regressor and evaluate using R-Squared Score."
      ],
      "metadata": {
        "id": "TDf2Q7vf3spM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Create a sample regression dataset\n",
        "X_reg, y_reg = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Regressor\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=100)\n",
        "gb_regressor.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Predict and evaluate R-Squared Score\n",
        "y_reg_pred = gb_regressor.predict(X_test_reg)\n",
        "r_squared = r2_score(y_test_reg, y_reg_pred)\n",
        "print(f\"Gradient Boosting Regressor R-Squared Score: {r_squared:.2f}\")"
      ],
      "metadata": {
        "id": "d1xurOTe9HXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting.\n"
      ],
      "metadata": {
        "id": "xCXzH9GB3u90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "xgb_classifier = xgb.XGBClassifier(n_estimators=100)\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_xgb_pred = xgb_classifier.predict(X_test)\n",
        "xgb_accuracy = accuracy_score(y_test, y_xgb_pred)\n",
        "\n",
        "# Compare with Gradient Boosting accuracy\n",
        "print(f\"XGBoost Classifier Accuracy: {xgb_accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "ZkTIJSA89Koh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Train a CatBoost Classifier and evaluate using F1-Score.\n"
      ],
      "metadata": {
        "id": "dwQfA0U_3ztU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Train CatBoost Classifier\n",
        "cat_classifier = CatBoostClassifier(iterations=100, verbose=0)\n",
        "cat_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate F1-Score\n",
        "y_cat_pred = cat_classifier.predict(X_test)\n",
        "f1 = f1_score(y_test, y_cat_pred)\n",
        "print(f\"CatBoost Classifier F1-Score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "GntLanmT9Nkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE).\n"
      ],
      "metadata": {
        "id": "DN2I5fVG32rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        " ```python\n",
        "# Train XGBoost Regressor\n",
        "xgb_regressor = xgb.XGBRegressor(n_estimators=100)\n",
        "xgb_regressor.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Predict and evaluate MSE\n",
        "y_xgb_reg_pred = xgb_regressor.predict(X_test_reg)\n",
        "mse = mean_squared_error(y_test_reg, y_xgb_reg_pred)\n",
        "print(f\"XGBoost Regressor MSE: {mse:.2f}\")"
      ],
      "metadata": {
        "id": "MRo1mYV59QC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Train an AdaBoost Classifier and visualize feature importance.\n"
      ],
      "metadata": {
        "id": "dFAh9NDn34o0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Train AdaBoost Classifier\n",
        "ada_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = ada_classifier.feature_importances_\n",
        "\n",
        "# Visualize feature importance\n",
        "plt.bar(range(len(feature_importance)), feature_importance)\n",
        "plt.title('Feature Importance - AdaBoost Classifier')\n",
        "plt.xlabel('Feature Index')\n",
        "plt.ylabel('Importance Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AsBnPqG49UNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Train a Gradient Boosting Regressor and plot learning curves.\n"
      ],
      "metadata": {
        "id": "Y_7py5gj36e-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "# Train Gradient Boosting Regressor\n",
        "gb_regressor.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Generate learning curves\n",
        "train_sizes, train_scores, test_scores = learning_curve(gb_regressor, X_reg, y_reg, cv=5)\n",
        "\n",
        "# Calculate mean and standard deviation\n",
        "train_mean = train_scores.mean(axis=1)\n",
        "test_mean = test_scores.mean(axis=1)\n",
        "\n",
        "# Plot learning curves\n",
        "plt.plot(train_sizes, train_mean, label='Training Score')\n",
        "plt.plot(train_sizes, test_mean, label='Cross-Validation Score')\n",
        "plt.title('Learning Curves - Gradient Boosting Regressor')\n",
        "plt.xlabel('Training Size')\n",
        "plt.ylabel('Score')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qjT8iWVe9V88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Train an XGBoost Classifier and visualize feature importance.\n"
      ],
      "metadata": {
        "id": "NY_YpAo3381W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train XGBoost Classifier\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance\n",
        "xgb_importance = xgb_classifier.feature_importances_\n",
        "\n",
        "# Visualize feature importance\n",
        "plt.bar(range(len(xgb_importance)), xgb_importance)\n",
        "plt.title('Feature Importance - XGBoost Classifier')\n",
        "plt.xlabel('Feature Index')\n",
        "plt.ylabel('Importance Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NpFOSdu09YwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Train a CatBoost Classifier and plot the confusion matrix.\n"
      ],
      "metadata": {
        "id": "Sq4Zp8Fc3-ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Train CatBoost Classifier\n",
        "cat_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict and compute confusion matrix\n",
        "y_cat_pred = cat_classifier.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_cat_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix - CatBoost Classifier')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HsznH9hL9b6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Train an AdaBoost Classifier with different numbers of estimators and compare accuracy.\n"
      ],
      "metadata": {
        "id": "UuW873GG4JrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store accuracies\n",
        "accuracies = []\n",
        "\n",
        "# Vary the number of estimators\n",
        "for n_estimators in [10, 50, 100, 200]:\n",
        "    ada_classifier = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=n_estimators)\n",
        "    ada_classifier.fit(X_train, y_train)\n",
        "    y_pred = ada_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append((n_estimators, accuracy))\n",
        "\n",
        "# Print accuracies\n",
        "for n_estimators, accuracy in accuracies:\n",
        "    print(f\"AdaBoost Classifier with {n_estimators} estimators Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "-GSNzS3R9fEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Train a Gradient Boosting Classifier and visualize the ROC curve.\n"
      ],
      "metadata": {
        "id": "IKPzsmu04LQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Train Gradient Boosting Classifier\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_prob = gb_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.title('Receiver Operating Characteristic - Gradient Boosting Classifier')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8G8_7nhN9iuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Train an XGBoost Regressor and tune the learning rate using GridSearchCV.\n"
      ],
      "metadata": {
        "id": "37zxeb4C4NO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {'learning_rate': [0.01, 0.1, 0.2], ' n_estimators': [100, 200]}\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(xgb.XGBRegressor(), param_grid, cv=5)\n",
        "grid_search.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Best parameters and score\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation score:\", grid_search.best_score_)"
      ],
      "metadata": {
        "id": "t7BKInHq9ky9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting.\n"
      ],
      "metadata": {
        "id": "9JZNIJn34PLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an imbalanced dataset\n",
        "X_imbalanced, y_imbalanced = make_classification(n_samples=1000, n_features=20, weights=[0.9, 0.1], random_state=42)\n",
        "X_train_imbalanced, X_test_imbalanced, y_train_imbalanced, y_test_imbalanced = train_test_split(X_imbalanced, y_imbalanced, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train CatBoost Classifier without class weighting\n",
        "cat_classifier = CatBoostClassifier(iterations=100, verbose=0)\n",
        "cat_classifier.fit(X_train_imbalanced, y_train_imbalanced)\n",
        "y_cat_pred = cat_classifier.predict(X_test_imbalanced)\n",
        "accuracy_no_weight = accuracy_score(y_test_imbalanced, y_cat_pred)\n",
        "\n",
        "# Train CatBoost Classifier with class weighting\n",
        "cat_classifier_weighted = CatBoostClassifier(iterations=100, class_weights=[1, 9], verbose=0)\n",
        "cat_classifier_weighted.fit(X_train_imbalanced, y_train_imbalanced)\n",
        "y_cat_pred_weighted = cat_classifier_weighted.predict(X_test_imbalanced)\n",
        "accuracy_weighted = accuracy_score(y_test_imbalanced, y_cat_pred_weighted)\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"Accuracy without class weighting: {accuracy_no_weight:.2f}\")\n",
        "print(f\"Accuracy with class weighting: {accuracy_weighted:.2f}\")"
      ],
      "metadata": {
        "id": "iGnrpceN9nQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Train an AdaBoost Classifier and analyze the effect of different learning rates.\n"
      ],
      "metadata": {
        "id": "DtXRy0g64RGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store accuracies for different learning rates\n",
        "learning_rates = [0.01, 0.1, 1.0]\n",
        "accuracies_lr = []\n",
        "\n",
        "# Vary the learning rate\n",
        "for lr in learning_rates:\n",
        "    ada_classifier = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, learning_rate=lr)\n",
        "    ada_classifier.fit(X_train, y_train)\n",
        "    y_pred = ada_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies_lr.append((lr, accuracy))\n",
        "\n",
        "# Print accuracies\n",
        "for lr, accuracy in accuracies_lr:\n",
        "    print(f\"AdaBoost Classifier with learning rate {lr} Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "qaWsMyZq9rYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Train an XGBoost Classifier for multi-class classification and evaluate using log-loss."
      ],
      "metadata": {
        "id": "iZFQYk4W4Tys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Create a multi-class dataset\n",
        "X_multi, y_multi = make_multilabel_classification(n_samples=1000, n_features=20, n_classes=3, random_state=42)\n",
        "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(X_multi, y_multi, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost Classifier for multi-class\n",
        "xgb_multi_classifier = xgb.XGBClassifier(objective='multi:softprob', n_estimators=100)\n",
        "xgb_multi_classifier.fit(X_train_multi, y_train_multi)\n",
        "\n",
        "# Predict probabilities and evaluate log-loss\n",
        "y_multi_prob = xgb_multi_classifier.predict_proba(X_test_multi)\n",
        "log_loss_value = log_loss(y_test_multi, y_multi_prob)\n",
        "print(f\"XGBoost Classifier Log-Loss: {log_loss_value:.2f}\")"
      ],
      "metadata": {
        "id": "aA2m0jgp9sOp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}